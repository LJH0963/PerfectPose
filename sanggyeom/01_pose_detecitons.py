import torch
import cv2
import numpy as np
from PIL import Image
from transformers import AutoProcessor, VitPoseForPoseEstimation

# ğŸ“Œ 1. GPU ì„¤ì • (ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ CUDA, ì•„ë‹ˆë©´ CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

# ğŸ“Œ 2. ViTPose ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
model_name = "usyd-community/vitpose-base-simple"
pose_processor = AutoProcessor.from_pretrained(model_name)
pose_model = VitPoseForPoseEstimation.from_pretrained(model_name).to(device)
pose_model.eval()

# ğŸ“Œ 3. ë™ì˜ìƒ ë¡œë“œ
video_path = "./tests/KSG/data/sample1_360.mp4"  # ì…ë ¥ ë™ì˜ìƒ íŒŒì¼
output_path = "./tests/KSG/data/sample1_360_out.mp4"  # ê²°ê³¼ ì €ì¥ íŒŒì¼

cap = cv2.VideoCapture(video_path)
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# COCO ë°ì´í„°ì…‹ ê¸°ì¤€ì˜ ê´€ì ˆ ì—°ê²° ì •ë³´
skeleton = [
    (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ” (ì˜¤ë¥¸ìª½, ì™¼ìª½)
    (11, 13), (13, 15), (12, 14), (14, 16),  # ë‹¤ë¦¬ (ì˜¤ë¥¸ìª½, ì™¼ìª½)
    (5, 6), (11, 12), (5, 11), (6, 12)  # ëª¸í†µ ì—°ê²°
]

# ğŸ“Œ 4. ë™ì˜ìƒ í”„ë ˆì„ë³„ ì²˜ë¦¬ ë£¨í”„
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ğŸ“Œ 4-1. OpenCV í”„ë ˆì„ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image_pil = Image.fromarray(image)

    # ğŸ“Œ 4-2. ViTPose ëª¨ë¸ ì…ë ¥ ì „ì²˜ë¦¬ (boxes ìˆ˜ì •)
    boxes = [[0, 0, frame_width, frame_height]]  # (x1, y1, x2, y2) í˜•ì‹
    inputs_pose = pose_processor(images=image_pil, boxes=boxes, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs_pose = pose_model(**inputs_pose)

    # í¬ì¦ˆ ë°ì´í„° í›„ì²˜ë¦¬
    pose_results = pose_processor.post_process_pose_estimation(outputs_pose, boxes=boxes, threshold=0.3)
    image_pose_result = pose_results[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ê²°ê³¼

    # ğŸ“Œ 4-3. í¬ì¦ˆ ì‹œê°í™”
    for person_pose in image_pose_result:
        for keypoint, label, score in zip(person_pose["keypoints"], person_pose["labels"], person_pose["scores"]):
            if score.item() > 0.5:  # ì‹ ë¢°ë„ 50% ì´ìƒì¸ ê²½ìš°ë§Œ ì‹œê°í™”
                x, y = int(keypoint[0].item()), int(keypoint[1].item())
                cv2.circle(frame, (x, y), 4, (0, 255, 0), -1)  # ê´€ì ˆ ì  ì‹œê°í™”

    # ğŸ“Œ 4-4. ê´€ì ˆ ì—°ê²°ì„  (ìŠ¤ì¼ˆë ˆí†¤) ì‹œê°í™”
    for pt1, pt2 in skeleton:
        if len(image_pose_result) > 0:
            keypoints = image_pose_result[0]["keypoints"]
            if keypoints[pt1][2] > 0.5 and keypoints[pt2][2] > 0.5:
                x1, y1 = int(keypoints[pt1][0]), int(keypoints[pt1][1])
                x2, y2 = int(keypoints[pt2][0]), int(keypoints[pt2][1])
                cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # ê´€ì ˆ ì—°ê²°ì„  ì‹œê°í™”

    # ğŸ“Œ 4-5. ê²°ê³¼ í”„ë ˆì„ ì €ì¥
    out.write(frame)

    # ğŸ“Œ 4-6. í™”ë©´ ì¶œë ¥ (ì‹¤ì‹œê°„ ë³´ê¸°, 'q' í‚¤ë¡œ ì¢…ë£Œ ê°€ëŠ¥)
    cv2.imshow('Pose Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# ğŸ“Œ 5. ë¦¬ì†ŒìŠ¤ í•´ì œ
cap.release()
out.release()
cv2.destroyAllWindows()
